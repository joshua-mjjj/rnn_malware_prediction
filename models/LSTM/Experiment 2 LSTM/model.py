# Author: Joshua Mwesigwa, shredakajoshua@gmail.com

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Dropout
from keras import optimizers
from itertools import cycle
import itertools


def plot_confusion_matrix(test_y, pred_y, class_names, filename):
    """
    This function prints and plots the confusion matrix.
    """
    cmap = plt.cm.Blues
    # Compute confusion matrix
    cm = confusion_matrix(
        np.argmax(test_y, axis=1), np.argmax(pred_y, axis=1))
    np.set_printoptions(precision=2)
    # Plot confusion matrix
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title("LSTM Confusion Matrix")
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)
    print(cm)
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.savefig(filename + ".png")
    # Plot normalized confusion matrix


def plotroc(test_y, pred_y, n_classes, filename):
    """
    Compute ROC curve and ROC area for each class
    """
    fpr = dict()
    tpr = dict()
    threshold = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], threshold[i] = roc_curve(test_y[:, i], pred_y[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    # Plot all ROC curves
    plt.figure()
    lw = 1
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
    for i, color in zip(range(n_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=lw,
                 label='ROC curve of class {0} (area = {1:0.2f})'''.
                 format(i, roc_auc[i]))
    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1])
    plt.ylim([0.0, 1])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('LSTM Performance')
    plt.legend(loc="lower right")
    plt.savefig(filename + '.png')
    return

def read_data(file_path):
    data = pd.read_csv(file_path, header=0)
    return data


def windows(data, window_size):
    start = 0
    while start < len(data):
        yield start, start + window_size - 1
        start += (window_size)


def extract_segments(data, window_size):
    segments = None
    labels = np.empty((0))
    for (start, end) in windows(data, window_size):
        if (len(data.loc[start:end]) == (window_size)):
            signal = np.asarray(data.loc[start:end])[:,1:11]
            if segments is None:
                segments = signal
            else:
                segments = np.vstack([segments, signal])
            labels = np.append(labels, data.loc[start:end]["malware"][start])
    return segments, labels


if __name__ == '__main__':

    """Hyperparameters"""
    win_size =19 
    num_var = 10
    split_ratio = 0.8

    print("Loading Data from file......")
    print("=============================")

    train_data = read_data("train.csv")
    
    print("Data loaded from file......")
    print("=============================")

    # Data preprocessing
    print("Preprocessing and standardizing data for model input......")
    print("=============================")

    segments, labels = extract_segments(train_data, win_size) 

    segments = segments.astype(float)

    labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)
    reshaped_segments = segments.reshape(
        [int(len(segments) / (win_size)), (win_size), num_var])

    # # """Create Train and Test Split based on split ratio"""

    train_test_split = np.random.rand(len(reshaped_segments)) < split_ratio
    print(len(train_test_split))
    train_x = reshaped_segments[train_test_split]
    train_y = labels[train_test_split]

    test_x = reshaped_segments[~train_test_split]
    test_y = labels[~train_test_split]

    def standardize(train, test):
        mean = np.mean(train, axis=0)
        std = np.std(train, axis=0)+0.000001

        train_x = (train - mean) / std 
        test_x = (test - mean) / std 
        return train_x, test_x

    train_x, test_x = standardize(train_x, test_x)

    print("Preprocessing Done......")
    print("=============================")

    print("Buiding model......")
    print("=============================")
    
    model = Sequential()
    model.add(LSTM(units=128, input_dim=num_var, input_length=win_size, return_sequences=True, dropout=0.2))
    model.add(LSTM(128, return_sequences=True, dropout=0.2))
    model.add(LSTM(64, return_sequences=True, dropout=0.2))
    model.add(LSTM(64, return_sequences=True, dropout=0.2))
    model.add(LSTM(64, return_sequences=False, dropout=0.2))
    model.add(Dense(2, activation='sigmoid'))
    opt = optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=opt, loss='binary_crossentropy',
                  metrics=['accuracy'])
    model.summary()
    
    print("Buiding Done......")
    print("=============================")
    print("Training model......")
    print("=============================")
    
    # Fit the network
    history = model.fit(train_x, 
                        train_y, 
                        epochs=40, 
                        batch_size=64,
                        verbose=1, 
                        # validation_split=0.1
                        validation_data=(test_x, test_y),
                    )

    print("Training DONE!!......")
    print("=============================")

    print("Saving weights!!......")
    print("=============================")

    # serialize model to JSON
    model_json = model.to_json()
    with open("lstm_model_1.json", "w") as json_file:
        json_file.write(model_json)
    # serialize weights to HDF5
    model.save_weights("lstm_model_1.h5")
    print("Saved model to disk.....")
 

    print("Representing Results......")
    print("=============================")

    # Plotting loss track 
    plt.plot(history.history['loss'], label='Training loss')
    plt.plot(history.history['val_loss'], label='Validation loss')
    plt.title('Model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend()
    plt.savefig("model_loss" + ".png")

    print(history.history.keys())
    # summarize history for accuracy
    plt.figure() 
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend()
    plt.savefig("model_accuracy" + ".png")


    # Predict Test Data and Plot ROC
    pred_y = model.predict(test_x, batch_size=64, verbose=2)
    plotroc(test_y, pred_y, 2, 'Testing ROC')
    class_names = ['Class 0, Benign', 'Class 1, Malware']
    plot_confusion_matrix(test_y, pred_y, class_names, 'test_confusion_matrix')

    pred_y = model.predict(train_x, batch_size=64, verbose=2)
    plotroc(train_y, pred_y, 2, 'Training ROC')
    plot_confusion_matrix(train_y, pred_y, class_names, 'train_confusion_matrix')

    

   
